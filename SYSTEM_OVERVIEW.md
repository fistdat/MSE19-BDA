# üèóÔ∏è Data Lakehouse System Overview

## üìã **Table of Contents**
- [System Architecture](#system-architecture)
- [Layer Descriptions](#layer-descriptions)
- [CDC Flow End-to-End](#cdc-flow-end-to-end)
- [Demo Guide](#demo-guide)
- [Tech Stack](#tech-stack)

---

## üèõÔ∏è **System Architecture**

### **Overall Architecture Diagram**
```mermaid
graph TB
    %% Source Layer
    subgraph "Source Layer"
        PG[(PostgreSQL)]
        PG --> |users table| PG
        PG --> |accounts table| PG
    end

    %% CDC Layer  
    subgraph "CDC & Streaming Layer"
        DZ[Debezium Connector]
        KAFKA[Apache Kafka]
        PG --> |Change Events| DZ
        DZ --> |CDC Events| KAFKA
    end

    %% Processing Layer
    subgraph "Stream Processing Layer"
        FM[Flink JobManager]
        FT[Flink TaskManager]
        FS[Flink SQL Client]
        KAFKA --> |JSON Events| FM
        FM --> FT
        FS --> FM
    end

    %% Storage Layer
    subgraph "Storage Layer"
        MINIO[MinIO Object Storage]
        ICE[Iceberg Tables]
        REST[Iceberg REST Catalog]
        FT --> |Parquet Files| MINIO
        FT --> |Metadata| ICE
        ICE --> REST
        MINIO --> ICE
    end

    %% Query Layer
    subgraph "Query & Analytics Layer"
        TRINO[Trino Query Engine]
        DBT[dbt Transformations]
        REST --> |Metadata| TRINO
        MINIO --> |Data Files| TRINO
        TRINO --> DBT
    end

    %% Visualization Layer
    subgraph "Visualization Layer"
        META[Metabase Dashboards]
        TRINO --> |Analytics Data| META
    end

    %% Orchestration Layer
    subgraph "Orchestration Layer"
        DAG[Dagster Orchestrator]
        DBT --> DAG
    end

    %% Styling
    classDef sourceStyle fill:#e1f5fe
    classDef cdcStyle fill:#f3e5f5
    classDef processStyle fill:#e8f5e8
    classDef storageStyle fill:#fff3e0
    classDef queryStyle fill:#fce4ec
    classDef vizStyle fill:#f1f8e9

    class PG sourceStyle
    class DZ,KAFKA cdcStyle
    class FM,FT,FS processStyle
    class MINIO,ICE,REST storageStyle
    class TRINO,DBT queryStyle
    class META,DAG vizStyle
```

### **Data Flow Architecture**
```mermaid
sequenceDiagram
    participant PG as PostgreSQL
    participant DZ as Debezium
    participant K as Kafka
    participant F as Flink
    participant I as Iceberg
    participant T as Trino
    participant M as Metabase

    Note over PG,M: Real-time CDC Data Pipeline

    PG->>DZ: INSERT/UPDATE/DELETE events
    DZ->>K: JSON CDC events with metadata
    K->>F: Stream processing
    F->>I: Structured data + metadata
    I->>T: Query interface
    T->>M: Analytics & visualizations
    
    Note over F,I: Checkpoint & metadata consistency
    Note over I,T: ACID transactions & time travel
```

---

## üèóÔ∏è **Layer Descriptions**

### **1. Source Layer - PostgreSQL**
**Purpose**: Primary data source v·ªõi transactional database  
**Components**:
- **PostgreSQL 14**: Bank database v·ªõi tables `users` v√† `accounts`
- **WAL (Write-Ahead Log)**: Transaction log cho CDC capture
- **Replication Slot**: Persistent CDC event streaming

**Key Features**:
- ACID transactions
- Logical replication enabled
- Debezium publication configured

---

### **2. CDC & Streaming Layer**

#### **Debezium Connector**
**Purpose**: Change Data Capture t·ª´ PostgreSQL  
**Features**:
- Captures INSERT, UPDATE, DELETE operations
- Preserves full transaction context
- Before/after state tracking

#### **Apache Kafka**
**Purpose**: Distributed streaming platform  
**Topics**:
- `bank.public.users`: User CDC events
- `bank.public.accounts`: Account CDC events
- High-throughput, fault-tolerant streaming

---

### **3. Stream Processing Layer - Apache Flink**

#### **Flink Components**
- **JobManager**: Job coordination v√† scheduling
- **TaskManager**: Parallel task execution  
- **SQL Client**: Interactive SQL interface

#### **Processing Logic**
- Real-time JSON parsing
- Schema extraction t·ª´ CDC events
- Iceberg table writes v·ªõi ACID guarantees
- Checkpoint-based fault tolerance

**SQL Transformations**:
```sql
-- Extract structured data from CDC JSON
SELECT
    CASE
        WHEN JSON_VALUE(payload, '$.op') = 'd'
            THEN JSON_QUERY(payload, '$.before')
        ELSE JSON_QUERY(payload, '$.after')
    END AS data,
    JSON_VALUE(payload, '$.op') AS op,
    CAST(JSON_VALUE(payload, '$.ts_ms') AS BIGINT) AS ts_ms
FROM kafka_source
```

---

### **4. Storage Layer - Iceberg + MinIO**

#### **Apache Iceberg**
**Purpose**: Open table format cho analytics  
**Features**:
- ACID transactions
- Schema evolution
- Time travel queries  
- Partition evolution
- Hidden partitioning

**Table Structure**:
```
iceberg.raw_data.users:
‚îú‚îÄ‚îÄ data: STRING (JSON CDC payload)
‚îú‚îÄ‚îÄ op: STRING (operation type: c/u/d)
‚îú‚îÄ‚îÄ ts_ms: BIGINT (event timestamp)
‚îú‚îÄ‚îÄ source_*: Metadata fields
‚îî‚îÄ‚îÄ Partitioned by date
```

#### **MinIO Object Storage**
**Purpose**: S3-compatible object storage  
**Structure**:
```
warehouse/
‚îú‚îÄ‚îÄ raw_data/
‚îÇ   ‚îú‚îÄ‚îÄ users/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ data/ (parquet files)
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ metadata/ (iceberg metadata)
‚îÇ   ‚îî‚îÄ‚îÄ accounts/
‚îî‚îÄ‚îÄ data_vault/ (future)
```

#### **Iceberg REST Catalog**
**Purpose**: Metadata management service  
**Functions**:
- Table metadata storage
- Schema registry
- Transaction coordination

---

### **5. Query & Analytics Layer**

#### **Trino Query Engine**
**Purpose**: Distributed SQL query engine  
**Capabilities**:
- Iceberg native integration
- Complex analytics queries
- Federation across data sources
- MPP (Massively Parallel Processing)

#### **dbt Transformations** (Planned)
**Purpose**: SQL-based data transformations  
**Models**:

**Parsed Data Layer**:
```sql
-- parsed_users.sql
SELECT 
    JSON_VALUE(data, '$.id') as user_id,
    JSON_VALUE(data, '$.email') as email,
    JSON_VALUE(data, '$.name') as name,
    op as operation_type,
    ts_ms as event_timestamp
FROM {{ source('raw_data', 'users') }}
```

**Data Vault Layer**:
- **Hubs**: Business keys (h_users, h_accounts)
- **Links**: Relationships (l_users_accounts)  
- **Satellites**: Descriptive data v·ªõi history tracking

---

### **6. Visualization Layer - Metabase**
**Purpose**: Business intelligence v√† dashboards  
**Features**:
- Starburst Trino connector
- Interactive dashboards
- Ad-hoc querying
- Business metrics visualization

**Connection**: `trino:8080/iceberg`

---

### **7. Orchestration Layer - Dagster** (Future)
**Purpose**: Data pipeline orchestration  
**Capabilities**:
- Asset-based orchestration
- dbt integration
- Lineage tracking
- Scheduling & monitoring

---

## üîÑ **CDC Flow End-to-End**

### **Data Flow Steps**

#### **Step 1: Source Data Changes**
```sql
-- PostgreSQL transaction
INSERT INTO users (email, name) VALUES ('user@example.com', 'New User');
UPDATE accounts SET balance = 15000.00 WHERE user_id = 'uuid';
DELETE FROM users WHERE id = 'uuid';
```

#### **Step 2: CDC Event Capture**
```json
{
  "op": "c",
  "after": {
    "id": "uuid",
    "email": "user@example.com", 
    "name": "New User",
    "created_at": "2024-01-01"
  },
  "source": {
    "connector": "postgresql",
    "ts_ms": 1640995200000,
    "db": "bank",
    "schema": "public",
    "table": "users"
  },
  "ts_ms": 1640995200000
}
```

#### **Step 3: Stream Processing** 
Flink SQL extracts v√† transforms:
- Parse JSON structure
- Extract business data
- Add processing metadata
- Write to Iceberg v·ªõi ACID guarantees

#### **Step 4: Storage & Cataloging**
- **Parquet files** written to MinIO
- **Metadata** registered v·ªõi Iceberg catalog
- **Schema evolution** handled automatically
- **Partitioning** by date for performance

#### **Step 5: Query & Analytics**
```sql
-- Trino analytics query
SELECT 
    DATE(from_unixtime(ts_ms/1000)) as event_date,
    op as operation_type,
    COUNT(*) as event_count
FROM iceberg.raw_data.users
GROUP BY 1, 2
ORDER BY event_date DESC;
```

---

## üéØ **Demo Guide**

### **Prerequisites**
```bash
# Start all services
docker compose up -d

# Verify services
docker ps
```

### **Demo Flow**

#### **1. Setup CDC Connector**
```bash
./run_bank_connector.sh
```
**Verify**: Check Debezium connector status
```bash
curl http://localhost:8083/connectors/bank-connector/status
```

#### **2. Generate Sample Data**
```bash
./run_psql_sql.sh
```
**Result**: Creates initial users v√† accounts

#### **3. Setup Flink Processing**
```bash
./run_flink_sql.sh  
```
**Verify**: Check Flink jobs
```bash
curl http://localhost:8081/jobs
```

#### **4. Trigger CDC Events**
```sql
-- Connect to PostgreSQL
docker exec -it postgres psql -U postgres -d bank

-- Insert new user
INSERT INTO users (email, name) VALUES ('demo@example.com', 'Demo User');

-- Update user
UPDATE users SET name = 'Demo User Updated' WHERE email = 'demo@example.com';

-- Check account balance
SELECT * FROM accounts LIMIT 5;

-- Update balance  
UPDATE accounts SET balance = balance + 1000 WHERE id = (SELECT id FROM accounts LIMIT 1);
```

#### **5. Verify Data Flow**

**Check Kafka Topics**:
```bash
docker exec -it kafka kafka-console-consumer.sh \
  --bootstrap-server kafka:29092 \
  --topic bank.public.users \
  --from-beginning --max-messages 5
```

**Check Iceberg Data**:
```sql
-- Connect to Trino
docker exec -it trino trino

-- Query raw CDC data
USE iceberg.raw_data;
SELECT COUNT(*) FROM users;
SELECT op, ts_ms FROM users ORDER BY ts_ms DESC LIMIT 5;

-- Analyze CDC operations
SELECT 
    op as operation_type,
    COUNT(*) as count
FROM users 
GROUP BY op;
```

**Check Data Files**:
```bash
# Check MinIO storage
ls -la minio/data/warehouse/raw_data/users/data/
ls -la minio/data/warehouse/raw_data/users/metadata/
```

#### **6. Analytics Queries**

**User Growth Analysis**:
```sql
SELECT 
    DATE(from_unixtime(ts_ms/1000)) as date,
    COUNT(CASE WHEN op = 'c' THEN 1 END) as new_users,
    COUNT(CASE WHEN op = 'u' THEN 1 END) as updated_users,
    COUNT(CASE WHEN op = 'd' THEN 1 END) as deleted_users
FROM iceberg.raw_data.users
GROUP BY 1
ORDER BY date DESC;
```

**Account Balance Tracking**:
```sql
WITH account_changes AS (
    SELECT 
        JSON_VALUE(data, '$.id') as account_id,
        JSON_VALUE(data, '$.balance') as balance,
        op,
        ts_ms
    FROM iceberg.raw_data.accounts
    WHERE op IN ('c', 'u')
)
SELECT 
    account_id,
    balance,
    ROW_NUMBER() OVER (PARTITION BY account_id ORDER BY ts_ms DESC) as rn
FROM account_changes
QUALIFY rn = 1;
```

#### **7. Metabase Dashboard**
1. Access Metabase: http://localhost:3001
2. Setup Starburst connection:
   - Host: `trino`  
   - Port: `8080`
   - Catalog: `iceberg`
   - Username: `commander`
3. Create dashboard v·ªõi CDC metrics

### **Expected Results**
- ‚úÖ Real-time CDC events flowing t·ª´ PostgreSQL
- ‚úÖ Data stored trong Iceberg parquet format
- ‚úÖ Analytics queries working trong Trino
- ‚úÖ Historical data tracking v·ªõi operation types
- ‚úÖ Performance optimized v·ªõi partitioning

---

## üõ†Ô∏è **Tech Stack**

### **Infrastructure**
- **Docker & Docker Compose**: Containerization
- **MinIO**: S3-compatible object storage

### **Data Sources**
- **PostgreSQL 14**: Transactional database

### **CDC & Streaming** 
- **Debezium**: Change data capture
- **Apache Kafka**: Event streaming platform

### **Stream Processing**
- **Apache Flink**: Real-time stream processing  
- **Flink SQL**: Declarative stream processing

### **Storage & Catalog**
- **Apache Iceberg**: Open table format
- **Iceberg REST Catalog**: Metadata management

### **Query & Transformation**
- **Trino**: Distributed SQL query engine
- **dbt**: SQL transformations (planned)

### **Orchestration** 
- **Dagster**: Data pipeline orchestration (planned)

### **Visualization**
- **Metabase**: Business intelligence dashboards

### **Languages & Formats**
- **SQL**: Query language
- **JSON**: CDC event format  
- **Parquet**: Columnar storage format

---

## üìä **Performance Characteristics**

### **Throughput**
- **CDC Latency**: < 100ms
- **Stream Processing**: 10K+ events/second
- **Query Performance**: Sub-second response

### **Scalability**
- **Horizontal**: Add Flink TaskManagers
- **Storage**: Unlimited v·ªõi object storage
- **Partitioning**: Date-based automatic partitioning

### **Reliability**
- **Fault Tolerance**: Flink checkpointing
- **ACID**: Iceberg transaction guarantees  
- **Durability**: Replicated object storage

---

## üîÆ **Future Enhancements**

### **Phase 2: Advanced Analytics**
- dbt Data Vault implementation
- Advanced SQL transformations
- Historical trend analysis

### **Phase 3: ML Integration** 
- Feature engineering pipelines
- Real-time ML inference
- Anomaly detection

### **Phase 4: Advanced Operations**
- Schema registry integration
- Advanced monitoring v·ªõi Prometheus
- Multi-tenant architecture

---

*üèóÔ∏è Data Lakehouse CDC Pipeline v1.0 - Real-time analytics v·ªõi historical tracking* 